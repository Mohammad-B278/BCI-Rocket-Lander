{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2486d67-3047-4b90-a22f-861fa344e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb407b-fead-4359-8f68-387b9480db60",
   "metadata": {},
   "source": [
    "## 1. Setup for Hyperparameter Tuning\n",
    "\n",
    "This notebook is dedicated to finding the optimal parameters for our BCI model. We will use data from a small, representative group of subjects to perform this search. The best parameters found here will be more robust than those from a single subject and will be used to train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76226bf-8f7e-46af-88e3-4f557dab3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup ---\n",
    "# Define a small group of subjects to tune on\n",
    "tuning_subjects = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15, 16, 17, 18, 19, 20] # Using 5 subjects is a good balance\n",
    "all_tuning_epochs = []\n",
    "print(f\"Loading data for {len(tuning_subjects)} subjects to run GridSearch...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f28f43-ca87-44c5-aef0-0bc9185572e0",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "As in the baseline notebook, we first need to load and prepare the data. The key difference here is that we are looping through a small group of subjects and collecting all their data together. This ensures that the parameters we find are not overly specialized to a single person's brain patterns. The preprocessing steps (filtering and epoching) remain identical to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4a9e0-6192-4d0f-87f3-77a51411c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load and Process Data for All Tuning Subjects ---\n",
    "for subject_id in tuning_subjects:\n",
    "    try:\n",
    "        runs_lr = [4, 8, 12]\n",
    "        runs_f = [6, 10, 14]\n",
    "        fnames_lr = eegbci.load_data(subject_id, runs=runs_lr, verbose=False)\n",
    "        fnames_f = eegbci.load_data(subject_id, runs=runs_f, verbose=False)\n",
    "\n",
    "        raw_lr = mne.concatenate_raws([mne.io.read_raw_edf(f, preload=True, verbose=False) for f in fnames_lr])\n",
    "        raw_f = mne.concatenate_raws([mne.io.read_raw_edf(f, preload=True, verbose=False) for f in fnames_f])\n",
    "\n",
    "        def process_and_epoch(raw, event_id_map, event_id_labels):\n",
    "            raw.filter(l_freq=8., h_freq=35., verbose=False)\n",
    "            events, _ = mne.events_from_annotations(raw, event_id=event_id_map, verbose=False)\n",
    "            epochs = mne.Epochs(raw, events, event_id_labels, tmin=-0.5, tmax=3.5, preload=True,\n",
    "                                baseline=None, picks='eeg', verbose=False)\n",
    "            epochs.resample(160., verbose=False)\n",
    "            return epochs\n",
    "\n",
    "        epochs_lr = process_and_epoch(raw_lr, {'T1': 1, 'T2': 2}, {'left_fist': 1, 'right_fist': 2})\n",
    "        epochs_f = process_and_epoch(raw_f, {'T2': 2}, {'both_feet': 2}) # Note: T1 is ignored here\n",
    "\n",
    "        all_tuning_epochs.append(mne.concatenate_epochs([epochs_lr, epochs_f], verbose=False))\n",
    "        print(f\"  Successfully processed subject {subject_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping subject {subject_id} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba35145-d0b9-47f5-badf-51c61516e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all epochs from all tuning subjects into one object\n",
    "final_tuning_epochs = mne.concatenate_epochs(all_tuning_epochs, verbose=False)\n",
    "\n",
    "# Now, extract the data and labels\n",
    "data = final_tuning_epochs.get_data()\n",
    "labels = final_tuning_epochs.events[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9f546-173c-4487-aafa-11e55fd49f20",
   "metadata": {},
   "source": [
    "## 3. The Riemannian Geometry Pipeline\n",
    "\n",
    "This pipeline replaces the traditional CSP feature extractor with a more modern approach based on Riemannian geometry. This method is often more powerful as it better respects the natural structure of EEG signals.\n",
    "\n",
    "### Step 1: Covariance Matrices\n",
    "For each epoch, we compute a **covariance matrix**. This matrix is a compact representation of all the spatial information in the EEG channelsâ€”i.e., how the signal from each electrode relates to every other electrode. The covariance matrix for a single epoch $X$ is calculated as:\n",
    "\n",
    "$$ C = \\frac{1}{n-1} X X^T $$\n",
    "\n",
    "- $X$ is the EEG data for one trial (channels x time points).\n",
    "- $n$ is the number of time points.\n",
    "\n",
    "### Step 2: Tangent Space Projection\n",
    "The space of all covariance matrices is not a standard \"flat\" Euclidean space; it's a curved manifold. To use standard classifiers like SVM, we project these matrices onto a \"flat\" hyperplane that is tangent to the manifold. This **Tangent Space** projection transforms the complex matrices into simple feature vectors that a standard SVM can classify effectively. This is the key feature extraction step of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4e291-b7af-438a-88ac-09ced2ae96a8",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Now we will search for the optimal settings for the SVM classifier that follows the Riemannian feature extraction. We use `GridSearchCV` to perform an exhaustive search over a grid of parameters, using cross-validation to find the combination that yields the highest accuracy.\n",
    "\n",
    "We will test:\n",
    "* **SVM Kernel**: Comparing a `linear` kernel to a non-linear `rbf` kernel.\n",
    "* **Regularization Strength (C)**: How much to penalize misclassified points.\n",
    "* **Kernel Coefficient (gamma)**: The influence of a single training example (for the `rbf` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1be2d-d3a1-4005-b192-fa58ec60f6c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "riemann_pipeline = Pipeline([\n",
    "    # Step 1: Calculate the covariance matrix for each epoch\n",
    "    ('Covariances', Covariances(estimator='lwf')),\n",
    "    # Step 2: Project the covariance matrices onto the tangent space\n",
    "    ('TangentSpace', TangentSpace(metric='riemann')),\n",
    "    # Step 3: Classify the resulting feature vectors with a placeholder SVM\n",
    "    ('Classifier', SVC()) # Use a placeholder SVC\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5955e-6843-444e-9927-95a8d52fcfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'Classifier__kernel': ['rbf', 'linear'],\n",
    "    'Classifier__C': [0.1, 1, 10, 100],\n",
    "    'Classifier__gamma': ['scale', 'auto', 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe636a-3ebd-4cf8-ae19-f7dd1017fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(riemann_pipeline, param_grid, cv=cv, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fc591-0315-4405-948e-c98caaf95091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters for Riemannian pipeline: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8d4e5-eeda-45b5-9155-da9943c21385",
   "metadata": {},
   "source": [
    "## 5. Tuning Results\n",
    "\n",
    "The grid search has completed. The results show the optimal combination of parameters for the Riemannian pipeline and the average cross-validation score achieved on our multi-subject tuning dataset.\n",
    "\n",
    "* **Best Parameters Found**:\n",
    "    * **Classifier**: Support Vector Machine (SVC) with a **Linear kernel**\n",
    "    * **SVM C**: 1\n",
    "* **Best Cross-Validation Score**: **77.28%**\n",
    "\n",
    "This demonstrates a significant improvement over our previous models. These optimal parameters will now be used to train the final model on the full training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64874a53-e634-43e9-8600-79014d331e4e",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
