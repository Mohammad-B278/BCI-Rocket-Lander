{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13807893-cd2c-476e-881b-ec01a80d5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from mne.decoding import CSP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4339c-a309-42d8-b702-1ca18b5af880",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration\n",
    "\n",
    "This section defines the parameters for our analysis. We specify the subject ID and the corresponding experimental run numbers for the different motor imagery tasks as detailed on the PhysioNet dataset description page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dee61-e0ce-4e12-8c3b-f995008b00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 1\n",
    "runs_lr = [4, 8, 12] # right or left hand fists data\n",
    "runs_feet = [6, 10, 14] # feet or both fists data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b30270-f06d-44b2-8c14-d855677ce335",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Here, we use the `mne.datasets.eegbci.load_data` function to automatically download the required `.edf` files from the public PhysioNet repository. We are downloading two distinct sets of experiments: one for left vs. right hand imagery and another that includes feet imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cc5f7-cf6e-4a30-93c8-f859815e6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading data...\")\n",
    "fnames_lr = eegbci.load_data(subject_id, runs = runs_lr, update_path = True)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de5ff1-12c1-47a5-858d-b8afb5e041df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading data...\")\n",
    "fnames_feet = eegbci.load_data(subject_id, runs = runs_feet, update_path = True)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abc4ce-f4df-4fdc-8930-af74061fb9ee",
   "metadata": {},
   "source": [
    "## 3. Reading and Combining Raw Data\n",
    "\n",
    "The downloaded `.edf` files are loaded into MNE's core data structure, the `Raw` object. Since each task (e.g., left/right fist) consists of multiple recording sessions (runs), we concatenate them to create a single, continuous data stream for each task type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4100968-6923-4b7c-944f-a98be9c02340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raws_lr = [mne.io.read_raw_edf(f, preload = True) for f in fnames_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18410d3f-f82a-4812-89e9-c834ab31a82c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raws_feet = [mne.io.read_raw_edf(f, preload = True) for f in fnames_feet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa1d19-e9b2-42ad-8d5c-0651d616d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate the list of Raw objects for each task into a single, continuous Raw object.\n",
    "raw_lr = mne.concatenate_raws(raws_lr)\n",
    "raw_feet = mne.concatenate_raws(raws_feet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f228e9-97d5-420e-8ca9-75215c7ce3ad",
   "metadata": {},
   "source": [
    "## 4. Signal Preprocessing and Epoching\n",
    "\n",
    "This is the most critical data cleaning and preparation stage.\n",
    "\n",
    "### Signal Filtering\n",
    "Raw EEG data is noisy. We apply two main filters:\n",
    "1.  **Band-Pass Filter (8-35 Hz):** We isolate the frequency bands most associated with motor control and imagery, known as the *mu* (μ) and *beta* (β) rhythms. This removes slow signal drifts and high-frequency noise.\n",
    "2.  **Notch Filter (50 Hz):** This specifically targets and removes electrical noise from the power grid, which is a common and powerful source of interference.\n",
    "\n",
    "### Epoching\n",
    "We slice the continuous signal into discrete time windows called **epochs** or **trials**. Each epoch is locked to a specific event (e.g., the cue to imagine moving the left fist). By creating these labeled trials, we transform the data into a format suitable for supervised machine learning. We use a time window from -1s to +4s to also capture the brain's preparatory activity before the cue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf405a05-9cf0-4cbc-b7b6-90d2fddc5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_epoch(raw, event_id):\n",
    "    raw.filter(l_freq=8., h_freq=35.)\n",
    "    raw.notch_filter(freqs=50)\n",
    "    \n",
    "    # Extract events from annotations. 'T1' and 'T2' are markers in the data\n",
    "    # corresponding to the start of different tasks.\n",
    "    events, _ = mne.events_from_annotations(raw, event_id={'T1': 1, 'T2': 2})\n",
    "\n",
    "    # starting 1 second before the cue to capture preparatory brain activity.\n",
    "    epochs = mne.Epochs(raw, events, event_id, tmin=-1., tmax=4., preload=True, baseline=None, picks='eeg')\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a50e4-73ba-4426-9311-47c88c54f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_lr = process_and_epoch(raw_lr, event_id={'left_fist': 1, 'right_fist': 2})\n",
    "epochs_feet = process_and_epoch(raw_feet, event_id={'feet': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde194fc-c5d5-4e6a-8303-0e42b04fb04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all processed epochs from all tasks into a single object for the classifier.\n",
    "epochs = mne.concatenate_epochs([epochs_lr, epochs_feet])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d5df7-f526-4576-b9f8-f190c7b183c9",
   "metadata": {},
   "source": [
    "## 5. Feature and Label Preparation\n",
    "\n",
    "We extract the final processed data and corresponding labels from the `Epochs` object. This prepares them for direct use with scikit-learn's machine learning models. The data is now a 3D NumPy array (`trials x channels x timepoints`), and the labels are a 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204da96a-d60b-4f64-b359-afef70774e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = epochs.get_data() # The data is a 3D numpy array: (n_epochs, n_channels, n_times).\n",
    "labels = epochs.events[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f189c-fb32-491f-b3d6-178324f622ac",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Pipeline: CSP + LDA\n",
    "\n",
    "To classify the high-dimensional EEG epochs, we build a pipeline that first extracts meaningful features and then uses a simple classifier.\n",
    "\n",
    "### Common Spatial Patterns (CSP)\n",
    "CSP is a feature extraction algorithm that finds spatial filters $W$ to maximize the variance for one class while minimizing it for another. This is ideal for discriminating between motor imagery tasks. The optimization problem it solves is maximizing the Rayleigh quotient:\n",
    "\n",
    "$$J(w) = \\frac{w^T \\Sigma_1 w}{w^T \\Sigma_2 w}$$\n",
    "\n",
    "- $w$ is the spatial filter (a set of weights for each channel).\n",
    "- $\\Sigma_1$ is the covariance matrix of the EEG data for the first class (e.g., left fist).\n",
    "- $\\Sigma_2$ is the covariance matrix of the EEG data for the second class (e.g., right fist).\n",
    "\n",
    "### Linear Discriminant Analysis (LDA)\n",
    "LDA is a linear classifier that projects the features from CSP onto a line to maximize the separation between the classes. The decision function for a new sample $x$ is:\n",
    "\n",
    "$$y(x) = w^T x + w_0$$\n",
    "\n",
    "### One-vs-Rest (OvR) Strategy\n",
    "Since CSP and LDA are inherently binary, we use the One-vs-Rest (OvR) strategy to handle our 3-class problem. It trains $K$ separate classifiers (where $K=3$ here). For a new sample $x$, the final prediction is the class $k$ whose classifier reports the highest confidence score:\n",
    "\n",
    "$$\\text{class} = \\arg\\max_{k \\in \\{1, ..., K\\}} f_k(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e8a2a-cb8a-41bf-9f4e-32425544e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "csp = CSP(n_components=4, reg=None, log=True) # n_components=4 means we are extracting the 4 most discriminative spatial patterns.\n",
    "lda = LDA()\n",
    "ovr_pipeline = OneVsRestClassifier(Pipeline([('CSP', csp), ('LDA', lda)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde53d6-39c4-439e-9a84-1be3d67361bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the cross-validation strategy. StratifiedKFold ensures each fold\n",
    "# has a representative balance of all three classes.\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(ovr_pipeline, data, labels, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c8e39-dd34-40bf-b266-781b076a3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 3-Class Cross-Validation Accuracy ---\")\n",
    "print(f\"Scores for each fold: {scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34700805-988e-4cc7-a5f5-45e9a26a44db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
